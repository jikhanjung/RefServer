#!/usr/bin/env python3
"""
GPU ê°ì§€ ìŠ¤í¬ë¦½íŠ¸
GPU ì‚¬ìš© ê°€ëŠ¥ì„±ì„ ì²´í¬í•˜ê³  ì ì ˆí•œ Docker Compose ì„¤ì •ì„ ê²°ì •í•©ë‹ˆë‹¤.
"""

import subprocess
import sys
import os
import platform
from typing import Dict, Any


def check_nvidia_gpu() -> Dict[str, Any]:
    """
    NVIDIA GPU ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
    
    Returns:
        dict: GPU ì •ë³´ ë° ì‚¬ìš© ê°€ëŠ¥ì„±
    """
    result = {
        "available": False,
        "driver_version": None,
        "gpu_count": 0,
        "gpu_names": [],
        "cuda_available": False,
        "docker_runtime": False
    }
    
    try:
        # nvidia-smi ëª…ë ¹ì–´ë¡œ GPU í™•ì¸
        nvidia_smi_result = subprocess.run(
            ["nvidia-smi", "--query-gpu=name,driver_version", "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if nvidia_smi_result.returncode == 0:
            lines = nvidia_smi_result.stdout.strip().split('\n')
            result["available"] = True
            result["gpu_count"] = len(lines)
            
            for line in lines:
                if line.strip():
                    parts = line.split(', ')
                    if len(parts) >= 2:
                        gpu_name = parts[0].strip()
                        driver_version = parts[1].strip()
                        result["gpu_names"].append(gpu_name)
                        if not result["driver_version"]:
                            result["driver_version"] = driver_version
                            
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        pass
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
    try:
        import torch
        result["cuda_available"] = torch.cuda.is_available()
    except ImportError:
        # PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° nvidia-ml-pyë¡œ í™•ì¸
        try:
            import pynvml
            pynvml.nvmlInit()
            result["cuda_available"] = True
        except ImportError:
            pass
    
    # Docker GPU ëŸ°íƒ€ì„ í™•ì¸
    try:
        docker_info = subprocess.run(
            ["docker", "info", "--format", "{{.Runtimes}}"],
            capture_output=True,
            text=True,
            timeout=10
        )
        if docker_info.returncode == 0 and "nvidia" in docker_info.stdout:
            result["docker_runtime"] = True
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        pass
    
    return result


def check_amd_gpu() -> Dict[str, Any]:
    """
    AMD GPU ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸ (ROCm)
    
    Returns:
        dict: AMD GPU ì •ë³´
    """
    result = {
        "available": False,
        "gpu_count": 0,
        "rocm_available": False
    }
    
    try:
        # rocm-smi í™•ì¸
        rocm_result = subprocess.run(
            ["rocm-smi", "--showproductname"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if rocm_result.returncode == 0:
            result["available"] = True
            # GPU ê°œìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ë°©ë²•)
            gpu_lines = [line for line in rocm_result.stdout.split('\n') if 'GPU' in line]
            result["gpu_count"] = len(gpu_lines)
            result["rocm_available"] = True
            
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        pass
    
    return result


def get_system_info() -> Dict[str, Any]:
    """
    ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
    
    Returns:
        dict: ì‹œìŠ¤í…œ ì •ë³´
    """
    return {
        "platform": platform.system(),
        "architecture": platform.machine(),
        "python_version": platform.python_version(),
        "docker_available": check_docker_available()
    }


def check_docker_available() -> bool:
    """
    Docker ì„¤ì¹˜ ë° ì‹¤í–‰ ìƒíƒœ í™•ì¸
    
    Returns:
        bool: Docker ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    """
    try:
        result = subprocess.run(
            ["docker", "version"],
            capture_output=True,
            text=True,
            timeout=10
        )
        return result.returncode == 0
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
        return False


def determine_deployment_mode() -> Dict[str, Any]:
    """
    ë°°í¬ ëª¨ë“œ ê²°ì •
    
    Returns:
        dict: ë°°í¬ ì„¤ì • ì •ë³´
    """
    nvidia_info = check_nvidia_gpu()
    amd_info = check_amd_gpu()
    system_info = get_system_info()
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ì„± ì¢…í•© íŒë‹¨
    gpu_available = nvidia_info["available"] or amd_info["available"]
    
    # ì„œë¹„ìŠ¤ë³„ í™œì„±í™” ì—¬ë¶€ ê²°ì •
    services = {
        "ollama_llama": True,  # Llama 3.2ëŠ” CPUì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥ (ë©”íƒ€ë°ì´í„° ì¶”ì¶œìš©)
        "ollama_llava": gpu_available and nvidia_info["docker_runtime"],  # LLaVAëŠ” GPU í•„ìš” (ì´ë¯¸ì§€ ì²˜ë¦¬)
        "huridocs": gpu_available,  # HuridocsëŠ” GPU ê°€ì† ê¶Œì¥
        "refserver": True  # RefServerëŠ” í•­ìƒ ì‹¤í–‰
    }
    
    # Docker Compose íŒŒì¼ ì„ íƒ
    if gpu_available:
        compose_file = "docker-compose.yml"
        deployment_mode = "gpu"
    else:
        compose_file = "docker-compose.cpu.yml"
        deployment_mode = "cpu"
    
    return {
        "deployment_mode": deployment_mode,
        "compose_file": compose_file,
        "services": services,
        "nvidia_info": nvidia_info,
        "amd_info": amd_info,
        "system_info": system_info,
        "recommendations": get_recommendations(nvidia_info, amd_info, system_info)
    }


def get_recommendations(nvidia_info: Dict, amd_info: Dict, system_info: Dict) -> list:
    """
    ì‚¬ìš©ìë¥¼ ìœ„í•œ ê¶Œì¥ì‚¬í•­ ìƒì„±
    
    Returns:
        list: ê¶Œì¥ì‚¬í•­ ëª©ë¡
    """
    recommendations = []
    
    if not nvidia_info["available"] and not amd_info["available"]:
        recommendations.append("GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ì „ìš© ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        recommendations.append("LLaVA (OCR í’ˆì§ˆ í‰ê°€)ì™€ Huridocs (ë ˆì´ì•„ì›ƒ ë¶„ì„) ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        recommendations.append("ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (Llama 3.2)ì€ CPUì—ì„œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        
    elif nvidia_info["available"] and not nvidia_info["docker_runtime"]:
        recommendations.append("NVIDIA GPUê°€ ê°ì§€ë˜ì—ˆì§€ë§Œ Docker GPU ëŸ°íƒ€ì„ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        recommendations.append("nvidia-docker2ë¥¼ ì„¤ì¹˜í•˜ì—¬ GPU ê°€ì†ì„ í™œìš©í•˜ì„¸ìš”.")
        recommendations.append("í˜„ì¬ëŠ” CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë©”íƒ€ë°ì´í„° ì¶”ì¶œì€ ì •ìƒ ì‘ë™).")
        
    elif nvidia_info["available"] and nvidia_info["docker_runtime"]:
        recommendations.append(f"NVIDIA GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ({nvidia_info['gpu_count']}ê°œ GPU ê°ì§€)")
        recommendations.append("ëª¨ë“  ì„œë¹„ìŠ¤ê°€ GPU ê°€ì†ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
    if not system_info["docker_available"]:
        recommendations.append("Dockerê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.")
        
    return recommendations


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” RefServer GPU ê°ì§€ ë° ë°°í¬ ëª¨ë“œ ê²°ì •\n")
    
    deployment_info = determine_deployment_mode()
    
    print(f"ğŸ“Š ë°°í¬ ëª¨ë“œ: {deployment_info['deployment_mode'].upper()}")
    print(f"ğŸ“„ Docker Compose íŒŒì¼: {deployment_info['compose_file']}")
    print()
    
    print("ğŸ¯ ì„œë¹„ìŠ¤ í™œì„±í™” ìƒíƒœ:")
    for service, enabled in deployment_info['services'].items():
        status = "âœ… í™œì„±í™”" if enabled else "âŒ ë¹„í™œì„±í™”"
        print(f"  - {service}: {status}")
    print()
    
    print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    for rec in deployment_info['recommendations']:
        print(f"  â€¢ {rec}")
    print()
    
    # í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì œì•ˆ
    env_vars = []
    if not deployment_info['services']['ollama_llava']:
        env_vars.append("LLAVA_ENABLED=false")
    if not deployment_info['services']['huridocs']:
        env_vars.append("HURIDOCS_LAYOUT_URL=disabled")
    
    if env_vars:
        print("ğŸ”§ ê¶Œì¥ í™˜ê²½ë³€ìˆ˜:")
        for var in env_vars:
            print(f"  export {var}")
        print()
    
    # ì‹¤í–‰ ëª…ë ¹ì–´ ì œì•ˆ
    print("ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´:")
    if deployment_info['deployment_mode'] == 'gpu':
        print(f"  docker-compose up --build")
    else:
        print(f"  docker-compose -f {deployment_info['compose_file']} up --build")
    
    return deployment_info


if __name__ == "__main__":
    import json
    deployment_info = main()
    
    # JSON íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥ (ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
    with open('.deployment_info.json', 'w') as f:
        json.dump(deployment_info, f, indent=2)